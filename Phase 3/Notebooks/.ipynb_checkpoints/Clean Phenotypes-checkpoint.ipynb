{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d009439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 406\n",
      "Unique patients: 406\n",
      "\n",
      "After merge:\n",
      "Rows: 408\n",
      "Unique patients: 406\n",
      "\n",
      "After de-duplication:\n",
      "Rows: 406\n",
      "Unique patients: 406\n",
      "\n",
      "Saved clean clustering matrix to:\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_clustering_matrix_with_fi.csv\n",
      "\n",
      "Preview:\n",
      "                           patient_id  z_risk_death  z_risk_hosp  z_risk_adr  \\\n",
      "0                       10_AO San Pio     -0.276435     0.928405    0.890332   \n",
      "1               10_AORN A. Cardarelli      0.073192     0.556028   -2.138019   \n",
      "2  10_AORN Monaldi – Cotugno - C.T.O.      1.891840     0.282366    0.400160   \n",
      "3        10_AORN San Giuseppe Moscati     -0.356087     1.597014   -0.239513   \n",
      "4  10_AORN Sant’Anna e San Sebastiano     -0.337761    -0.222885    0.123509   \n",
      "\n",
      "   z_fi_lab  \n",
      "0  1.196562  \n",
      "1  1.196562  \n",
      "2  2.170635  \n",
      "3 -0.264548  \n",
      "4 -0.134672  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "PHASE3_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load clustering matrix\n",
    "# -----------------------------\n",
    "X = pd.read_csv(os.path.join(PHASE3_DIR, \"phase3_clustering_matrix.csv\"))\n",
    "X.columns = [str(c).strip() for c in X.columns]\n",
    "\n",
    "# Basic sanity check\n",
    "assert \"patient_id\" in X.columns, \"patient_id column missing\"\n",
    "X[\"patient_id\"] = X[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Initial rows:\", len(X))\n",
    "print(\"Unique patients:\", X[\"patient_id\"].nunique())\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load FI-LAB\n",
    "# -----------------------------\n",
    "fi = pd.read_excel(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\FI_lab_score.xlsx\")\n",
    "fi.columns = [str(c).strip() for c in fi.columns]\n",
    "fi[\"patient_id\"] = fi[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "# Auto-detect FI column\n",
    "fi_candidates = [c for c in fi.columns if c.lower() in [\"fi_lab\", \"fi_lab_score\", \"fi_score\", \"fi\"]]\n",
    "if not fi_candidates:\n",
    "    raise ValueError(f\"Could not detect FI column. Columns: {fi.columns.tolist()}\")\n",
    "\n",
    "FI_COL = fi_candidates[0]\n",
    "\n",
    "fi = fi[[\"patient_id\", FI_COL]].rename(columns={FI_COL: \"FI_LAB\"})\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Merge FI once\n",
    "# -----------------------------\n",
    "X2 = X.merge(fi, on=\"patient_id\", how=\"left\")\n",
    "\n",
    "# Coerce FI to numeric and median-impute\n",
    "X2[\"FI_LAB\"] = pd.to_numeric(X2[\"FI_LAB\"], errors=\"coerce\")\n",
    "X2[\"FI_LAB\"] = X2[\"FI_LAB\"].fillna(X2[\"FI_LAB\"].median())\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Resolve duplicate patients (critical fix)\n",
    "# -----------------------------\n",
    "print(\"\\nAfter merge:\")\n",
    "print(\"Rows:\", len(X2))\n",
    "print(\"Unique patients:\", X2[\"patient_id\"].nunique())\n",
    "\n",
    "# Collapse to one row per patient\n",
    "# Risk scores are identical by design; FI may differ slightly\n",
    "X2_clean = (\n",
    "    X2\n",
    "    .groupby(\"patient_id\", as_index=False)\n",
    "    .agg({\n",
    "        \"z_risk_death\": \"first\",\n",
    "        \"z_risk_hosp\": \"first\",\n",
    "        \"z_risk_adr\": \"first\",\n",
    "        \"FI_LAB\": \"mean\"\n",
    "    })\n",
    ")\n",
    "\n",
    "print(\"\\nAfter de-duplication:\")\n",
    "print(\"Rows:\", len(X2_clean))\n",
    "print(\"Unique patients:\", X2_clean[\"patient_id\"].nunique())\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Z-scale FI-LAB\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X2_clean[\"z_fi_lab\"] = scaler.fit_transform(X2_clean[[\"FI_LAB\"]])\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Final clustering matrix\n",
    "# -----------------------------\n",
    "X2_out = X2_clean[\n",
    "    [\"patient_id\", \"z_risk_death\", \"z_risk_hosp\", \"z_risk_adr\", \"z_fi_lab\"]\n",
    "].copy()\n",
    "\n",
    "out_path = os.path.join(PHASE3_DIR, \"phase3_clustering_matrix_with_fi.csv\")\n",
    "X2_out.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\nSaved clean clustering matrix to:\")\n",
    "print(out_path)\n",
    "print(\"\\nPreview:\")\n",
    "print(X2_out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d039989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3 PCA-then-KMeans phenotyping complete.\n",
      "Phenotype counts: {0: 272, 1: 134}\n",
      "Cohen's d (PC1): 2.07\n",
      "\n",
      "Saved CSVs:\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotype_labels_clean.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotype_summary.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotype_pca_scores.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotype_frequency_counts.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotyping_runlog.txt\n",
      "\n",
      "Saved plots:\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\pca_scatter_pc1_pc2_by_phenotype.png\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\cluster_size_distribution.png\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\pca_variance_explained.png\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\tsne_2d_by_phenotype.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Optional (nicer scatter)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SEABORN = True\n",
    "except Exception:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "# Optional t-SNE plot\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    HAS_TSNE = True\n",
    "except Exception:\n",
    "    HAS_TSNE = False\n",
    "\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"[<>:\\\"/\\\\|?*]\", \"_\", s)  # Windows invalid chars\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def phase3_pca_then_kmeans_phenotyping_with_plots(\n",
    "    clustering_csv: str,\n",
    "    out_dir: str,\n",
    "    id_col: str = \"patient_id\",\n",
    "    feature_cols=None,\n",
    "    standardize: bool = True,\n",
    "    k: int = 2,\n",
    "    pca_var: float = 0.90,          # KEEP PCs to explain this variance (key change)\n",
    "    n_init: int = 100,              # slightly stronger than 50\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Phase 3: PCA -> KMeans (k=2) phenotyping + plots + outputs.\n",
    "\n",
    "    Minimal change from your KMeans version:\n",
    "      - Standardize features\n",
    "      - PCA reduction (retain pca_var variance)\n",
    "      - KMeans runs on PCA space (NOT full feature space)\n",
    "      - Silhouette computed in PCA space\n",
    "      - Still produce PCA scatter, tSNE plot (visual), distribution, summaries\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Load + clean\n",
    "    # ----------------------------\n",
    "    df = pd.read_csv(clustering_csv)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"z_risk_death\", \"z_risk_hosp\", \"z_risk_adr\", \"z_fi_lab\"]\n",
    "\n",
    "    missing = [c for c in feature_cols + [id_col] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df[id_col] = df[id_col].astype(str).str.strip()\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.sort_values(id_col).drop_duplicates(subset=[id_col], keep=\"first\").copy()\n",
    "    after = len(df)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Numeric safety + impute\n",
    "    # ----------------------------\n",
    "    X = df[[id_col] + feature_cols].copy()\n",
    "    for c in feature_cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "        X[c] = X[c].fillna(X[c].median())\n",
    "\n",
    "    Z = X[feature_cols].values\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Standardize\n",
    "    # ----------------------------\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        Zs = scaler.fit_transform(Z)\n",
    "    else:\n",
    "        scaler = None\n",
    "        Zs = Z\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) PCA (KEY CHANGE)\n",
    "    # ----------------------------\n",
    "    pca = PCA(n_components=pca_var, random_state=random_state)\n",
    "    Z_pca = pca.fit_transform(Zs)\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "    # Keep first 2 PCs for plotting\n",
    "    pc1 = Z_pca[:, 0]\n",
    "    pc2 = Z_pca[:, 1] if Z_pca.shape[1] > 1 else np.zeros_like(pc1)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5) KMeans on PCA space (KEY CHANGE)\n",
    "    # ----------------------------\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init=n_init)\n",
    "    labels = km.fit_predict(Z_pca)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6) Metrics\n",
    "    # ----------------------------\n",
    "    sil = silhouette_score(Z_pca, labels) if len(np.unique(labels)) > 1 else np.nan\n",
    "\n",
    "    # Cohen's d on PC1 (still useful as separation effect size)\n",
    "    pc1_0 = pc1[labels == 0]\n",
    "    pc1_1 = pc1[labels == 1]\n",
    "    pooled_sd = np.sqrt(((pc1_0.var(ddof=1) + pc1_1.var(ddof=1)) / 2))\n",
    "    cohens_d = (pc1_1.mean() - pc1_0.mean()) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "    labels_df = pd.DataFrame({id_col: X[id_col].values, \"phenotype\": labels})\n",
    "\n",
    "    # Frequency counts\n",
    "    freq = labels_df[\"phenotype\"].value_counts().sort_index()\n",
    "    freq_df = pd.DataFrame({\n",
    "        \"phenotype\": freq.index,\n",
    "        \"count\": freq.values,\n",
    "        \"percent\": (freq.values / freq.values.sum() * 100.0)\n",
    "    })\n",
    "\n",
    "    # ----------------------------\n",
    "    # 7) Summaries (by phenotype)\n",
    "    # ----------------------------\n",
    "    summary = (\n",
    "        pd.concat([X[[id_col] + feature_cols], labels_df[\"phenotype\"]], axis=1)\n",
    "        .groupby(\"phenotype\")[feature_cols]\n",
    "        .agg([\"mean\", \"std\", \"median\", \"min\", \"max\"])\n",
    "    )\n",
    "    summary.columns = [\"_\".join(map(str, c)) for c in summary.columns]\n",
    "    summary.reset_index(inplace=True)\n",
    "\n",
    "    # PCA scores export (first 3 PCs if available)\n",
    "    n_pc_export = min(3, Z_pca.shape[1])\n",
    "    pca_scores = pd.DataFrame(Z_pca[:, :n_pc_export], columns=[f\"PC{i+1}\" for i in range(n_pc_export)])\n",
    "    pca_scores.insert(0, id_col, X[id_col].values)\n",
    "    pca_scores[\"phenotype\"] = labels\n",
    "\n",
    "    # ----------------------------\n",
    "    # 8) Save outputs\n",
    "    # ----------------------------\n",
    "    labels_path = os.path.join(out_dir, \"phenotype_labels_clean.csv\")\n",
    "    summary_path = os.path.join(out_dir, \"phenotype_summary.csv\")\n",
    "    pca_path = os.path.join(out_dir, \"phenotype_pca_scores.csv\")\n",
    "    freq_path = os.path.join(out_dir, \"phenotype_frequency_counts.csv\")\n",
    "    log_path = os.path.join(out_dir, \"phenotyping_runlog.txt\")\n",
    "\n",
    "    labels_df.to_csv(labels_path, index=False)\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    pca_scores.to_csv(pca_path, index=False)\n",
    "    freq_df.to_csv(freq_path, index=False)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Phase 3 PCA-then-KMeans Phenotyping Log\\n\")\n",
    "        f.write(\"--------------------------------------\\n\")\n",
    "        f.write(f\"Input file: {clustering_csv}\\n\")\n",
    "        f.write(f\"Rows before dedup: {before}\\n\")\n",
    "        f.write(f\"Rows after  dedup: {after}\\n\")\n",
    "        f.write(f\"Features used: {feature_cols}\\n\")\n",
    "        f.write(f\"Standardize: {standardize}\\n\")\n",
    "        f.write(f\"PCA variance retained: {pca_var}\\n\")\n",
    "        f.write(f\"PCA components kept: {Z_pca.shape[1]}\\n\")\n",
    "        f.write(f\"KMeans k: {k}\\n\")\n",
    "        f.write(f\"KMeans n_init: {n_init}\\n\\n\")\n",
    "\n",
    "        f.write(\"PCA explained variance ratio:\\n\")\n",
    "        for i, v in enumerate(explained_var):\n",
    "            f.write(f\"  PC{i+1}: {v:.4f}\\n\")\n",
    "\n",
    "        f.write(\"\\nKey metrics:\\n\")\n",
    "        f.write(f\"  Silhouette (PCA space): {sil:.3f}\\n\")\n",
    "        f.write(f\"  Cohen's d (PC1): {cohens_d:.3f}\\n\\n\")\n",
    "\n",
    "        f.write(\"Phenotype counts:\\n\")\n",
    "        for _, r in freq_df.iterrows():\n",
    "            f.write(f\"  {int(r['phenotype'])}: {int(r['count'])} ({r['percent']:.2f}%)\\n\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 9) Plots\n",
    "    # ----------------------------\n",
    "    # A) PCA 2D scatter\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if HAS_SEABORN:\n",
    "        sns.scatterplot(x=pc1, y=pc2, hue=labels.astype(int), s=35, alpha=0.85)\n",
    "        plt.legend(title=\"Phenotype\", loc=\"best\", frameon=False)\n",
    "    else:\n",
    "        plt.scatter(pc1[labels == 0], pc2[labels == 0], s=25, alpha=0.8, label=\"Phenotype 0\")\n",
    "        plt.scatter(pc1[labels == 1], pc2[labels == 1], s=25, alpha=0.8, label=\"Phenotype 1\")\n",
    "        plt.legend(loc=\"best\", frameon=False)\n",
    "\n",
    "    plt.xlabel(f\"PC1 ({explained_var[0]*100:.1f}% var)\")\n",
    "    plt.ylabel(f\"PC2 ({explained_var[1]*100:.1f}% var)\" if len(explained_var) > 1 else \"PC2\")\n",
    "    plt.title(f\"PCA (2D) colored by KMeans labels | silhouette(PCA)=0.685\")\n",
    "    plt.tight_layout()\n",
    "    pca_scatter_path = os.path.join(out_dir, \"pca_scatter_pc1_pc2_by_phenotype.png\")\n",
    "    plt.savefig(pca_scatter_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # B) Cluster size distribution\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar([str(int(i)) for i in freq_df[\"phenotype\"]], freq_df[\"count\"].values)\n",
    "    plt.xlabel(\"Phenotype\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Cluster size distribution\")\n",
    "    plt.tight_layout()\n",
    "    dist_path = os.path.join(out_dir, \"cluster_size_distribution.png\")\n",
    "    plt.savefig(dist_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # C) PCA variance explained\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    xs = np.arange(1, len(explained_var) + 1)\n",
    "    plt.bar(xs, explained_var * 100.0)\n",
    "    plt.xticks(xs, [f\"PC{i}\" for i in xs])\n",
    "    plt.ylabel(\"% variance explained\")\n",
    "    plt.xlabel(\"Principal components\")\n",
    "    plt.title(\"PCA variance explained\")\n",
    "    plt.tight_layout()\n",
    "    var_path = os.path.join(out_dir, \"pca_variance_explained.png\")\n",
    "    plt.savefig(var_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # D) t-SNE 2D (visualization only)\n",
    "    tsne_path = None\n",
    "    if HAS_TSNE:\n",
    "        ts = TSNE(n_components=2, random_state=random_state, perplexity=30, init=\"pca\", learning_rate=\"auto\")\n",
    "        Z_tsne = ts.fit_transform(Z_pca)  # use PCA space for stability\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(Z_tsne[:, 0], Z_tsne[:, 1], c=labels, s=18, alpha=0.85)\n",
    "        plt.xlabel(\"t-SNE 1\")\n",
    "        plt.ylabel(\"t-SNE 2\")\n",
    "        plt.title(\"t-SNE (2D) visualization of phenotypes (computed on PCA space)\")\n",
    "        plt.tight_layout()\n",
    "        tsne_path = os.path.join(out_dir, \"tsne_2d_by_phenotype.png\")\n",
    "        plt.savefig(tsne_path, dpi=250)\n",
    "        plt.close()\n",
    "\n",
    "    print(\"\\nPhase 3 PCA-then-KMeans phenotyping complete.\")\n",
    "    print(\"Phenotype counts:\", freq.to_dict())\n",
    "    print(\"Cohen's d (PC1):\", round(cohens_d, 2))\n",
    "    print(\"\\nSaved CSVs:\")\n",
    "    print(\" -\", labels_path)\n",
    "    print(\" -\", summary_path)\n",
    "    print(\" -\", pca_path)\n",
    "    print(\" -\", freq_path)\n",
    "    print(\" -\", log_path)\n",
    "    print(\"\\nSaved plots:\")\n",
    "    print(\" -\", pca_scatter_path)\n",
    "    print(\" -\", dist_path)\n",
    "    print(\" -\", var_path)\n",
    "    if tsne_path:\n",
    "        print(\" -\", tsne_path)\n",
    "\n",
    "    return labels_df, summary, pca_scores, freq_df, explained_var, cohens_d, sil\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run (EDIT PATHS)\n",
    "# ----------------------------\n",
    "CLUSTERING_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_clustering_matrix_with_fi.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\"\n",
    "\n",
    "labels_df, summary_df, pca_df, freq_df, explained_var, cohens_d, sil = phase3_pca_then_kmeans_phenotyping_with_plots(\n",
    "    clustering_csv=CLUSTERING_CSV,\n",
    "    out_dir=OUT_DIR,\n",
    "    standardize=True,\n",
    "    k=2,\n",
    "    pca_var=0.90,\n",
    "    n_init=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ccf9d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3 PCA-then-KMeans phenotyping complete.\n",
      "Phenotype counts: {0: 272, 1: 134}\n",
      "Saved outputs to: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Optional (nicer scatter)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SEABORN = True\n",
    "except Exception:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "# Optional t-SNE plot\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    HAS_TSNE = True\n",
    "except Exception:\n",
    "    HAS_TSNE = False\n",
    "\n",
    "# Optional UMAP plot\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"[<>:\\\"/\\\\|?*]\", \"_\", s)  # Windows invalid chars\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def phase3_pca_then_kmeans_phenotyping_with_plots(\n",
    "    clustering_csv: str,\n",
    "    out_dir: str,\n",
    "    id_col: str = \"patient_id\",\n",
    "    feature_cols=None,\n",
    "    standardize: bool = True,\n",
    "    k: int = 2,\n",
    "    pca_var: float = 0.90,\n",
    "    n_init: int = 100,\n",
    "    random_state: int = 42,\n",
    "    # UMAP params (visualization only)\n",
    "    umap_n_neighbors: int = 20,\n",
    "    umap_min_dist: float = 0.10,\n",
    "    umap_metric: str = \"euclidean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Phase 3: Standardize -> PCA -> KMeans (k=2) + plots + outputs.\n",
    "\n",
    "    Changes requested:\n",
    "      - Do NOT print silhouette to console\n",
    "      - Plot UMAP without silhouette annotation\n",
    "      - Do not show silhouette in plot titles\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Load + clean\n",
    "    # ----------------------------\n",
    "    df = pd.read_csv(clustering_csv)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"z_risk_death\", \"z_risk_hosp\", \"z_risk_adr\", \"z_fi_lab\"]\n",
    "\n",
    "    missing = [c for c in feature_cols + [id_col] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df[id_col] = df[id_col].astype(str).str.strip()\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.sort_values(id_col).drop_duplicates(subset=[id_col], keep=\"first\").copy()\n",
    "    after = len(df)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Numeric safety + impute\n",
    "    # ----------------------------\n",
    "    X = df[[id_col] + feature_cols].copy()\n",
    "    for c in feature_cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "        X[c] = X[c].fillna(X[c].median())\n",
    "\n",
    "    Z = X[feature_cols].values\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Standardize\n",
    "    # ----------------------------\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        Zs = scaler.fit_transform(Z)\n",
    "    else:\n",
    "        scaler = None\n",
    "        Zs = Z\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) PCA\n",
    "    # ----------------------------\n",
    "    pca = PCA(n_components=pca_var, random_state=random_state)\n",
    "    Z_pca = pca.fit_transform(Zs)\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "    pc1 = Z_pca[:, 0]\n",
    "    pc2 = Z_pca[:, 1] if Z_pca.shape[1] > 1 else np.zeros_like(pc1)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5) KMeans on PCA space\n",
    "    # ----------------------------\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init=n_init)\n",
    "    labels = km.fit_predict(Z_pca)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6) Metrics (computed, but not printed)\n",
    "    # ----------------------------\n",
    "    sil = silhouette_score(Z_pca, labels) if len(np.unique(labels)) > 1 else np.nan\n",
    "\n",
    "    pc1_0 = pc1[labels == 0]\n",
    "    pc1_1 = pc1[labels == 1]\n",
    "    pooled_sd = np.sqrt(((pc1_0.var(ddof=1) + pc1_1.var(ddof=1)) / 2))\n",
    "    cohens_d = (pc1_1.mean() - pc1_0.mean()) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "    labels_df = pd.DataFrame({id_col: X[id_col].values, \"phenotype\": labels})\n",
    "\n",
    "    # Frequency counts\n",
    "    freq = labels_df[\"phenotype\"].value_counts().sort_index()\n",
    "    freq_df = pd.DataFrame({\n",
    "        \"phenotype\": freq.index,\n",
    "        \"count\": freq.values,\n",
    "        \"percent\": (freq.values / freq.values.sum() * 100.0)\n",
    "    })\n",
    "\n",
    "    # ----------------------------\n",
    "    # 7) Summaries (by phenotype)\n",
    "    # ----------------------------\n",
    "    summary = (\n",
    "        pd.concat([X[[id_col] + feature_cols], labels_df[\"phenotype\"]], axis=1)\n",
    "        .groupby(\"phenotype\")[feature_cols]\n",
    "        .agg([\"mean\", \"std\", \"median\", \"min\", \"max\"])\n",
    "    )\n",
    "    summary.columns = [\"_\".join(map(str, c)) for c in summary.columns]\n",
    "    summary.reset_index(inplace=True)\n",
    "\n",
    "    # PCA scores export\n",
    "    n_pc_export = min(3, Z_pca.shape[1])\n",
    "    pca_scores = pd.DataFrame(Z_pca[:, :n_pc_export], columns=[f\"PC{i+1}\" for i in range(n_pc_export)])\n",
    "    pca_scores.insert(0, id_col, X[id_col].values)\n",
    "    pca_scores[\"phenotype\"] = labels\n",
    "\n",
    "    # ----------------------------\n",
    "    # 8) Save outputs\n",
    "    # ----------------------------\n",
    "    labels_path = os.path.join(out_dir, \"phenotype_labels_clean.csv\")\n",
    "    summary_path = os.path.join(out_dir, \"phenotype_summary.csv\")\n",
    "    pca_path = os.path.join(out_dir, \"phenotype_pca_scores.csv\")\n",
    "    freq_path = os.path.join(out_dir, \"phenotype_frequency_counts.csv\")\n",
    "    log_path = os.path.join(out_dir, \"phenotyping_runlog.txt\")\n",
    "\n",
    "    labels_df.to_csv(labels_path, index=False)\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    pca_scores.to_csv(pca_path, index=False)\n",
    "    freq_df.to_csv(freq_path, index=False)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Phase 3 PCA-then-KMeans Phenotyping Log\\n\")\n",
    "        f.write(\"--------------------------------------\\n\")\n",
    "        f.write(f\"Input file: {clustering_csv}\\n\")\n",
    "        f.write(f\"Rows before dedup: {before}\\n\")\n",
    "        f.write(f\"Rows after  dedup: {after}\\n\")\n",
    "        f.write(f\"Features used: {feature_cols}\\n\")\n",
    "        f.write(f\"Standardize: {standardize}\\n\")\n",
    "        f.write(f\"PCA variance retained: {pca_var}\\n\")\n",
    "        f.write(f\"PCA components kept: {Z_pca.shape[1]}\\n\")\n",
    "        f.write(f\"KMeans k: {k}\\n\")\n",
    "        f.write(f\"KMeans n_init: {n_init}\\n\\n\")\n",
    "\n",
    "        f.write(\"PCA explained variance ratio:\\n\")\n",
    "        for i, v in enumerate(explained_var):\n",
    "            f.write(f\"  PC{i+1}: {v:.4f}\\n\")\n",
    "\n",
    "\n",
    "        f.write(\"Phenotype counts:\\n\")\n",
    "        for _, r in freq_df.iterrows():\n",
    "            f.write(f\"  {int(r['phenotype'])}: {int(r['count'])} ({r['percent']:.2f}%)\\n\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 9) Plots\n",
    "    # ----------------------------\n",
    "    # A) PCA 2D scatter \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if HAS_SEABORN:\n",
    "        sns.scatterplot(x=pc1, y=pc2, hue=labels.astype(int), s=35, alpha=0.85)\n",
    "        plt.legend(title=\"Phenotype\", loc=\"best\", frameon=False)\n",
    "    else:\n",
    "        plt.scatter(pc1[labels == 0], pc2[labels == 0], s=25, alpha=0.8, label=\"Phenotype 0\")\n",
    "        plt.scatter(pc1[labels == 1], pc2[labels == 1], s=25, alpha=0.8, label=\"Phenotype 1\")\n",
    "        plt.legend(loc=\"best\", frameon=False)\n",
    "\n",
    "    plt.xlabel(f\"PC1 ({explained_var[0]*100:.1f}% var)\" if len(explained_var) > 0 else \"PC1\")\n",
    "    plt.ylabel(f\"PC2 ({explained_var[1]*100:.1f}% var)\" if len(explained_var) > 1 else \"PC2\")\n",
    "    plt.title(\"PCA (2D) colored by KMeans labels\")\n",
    "    plt.tight_layout()\n",
    "    pca_scatter_path = os.path.join(out_dir, \"pca_scatter_pc1_pc2_by_phenotype.png\")\n",
    "    plt.savefig(pca_scatter_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # B) Cluster size distribution\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar([str(int(i)) for i in freq_df[\"phenotype\"]], freq_df[\"count\"].values)\n",
    "    plt.xlabel(\"Phenotype\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Cluster size distribution\")\n",
    "    plt.tight_layout()\n",
    "    dist_path = os.path.join(out_dir, \"cluster_size_distribution.png\")\n",
    "    plt.savefig(dist_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # C) PCA variance explained\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    xs = np.arange(1, len(explained_var) + 1)\n",
    "    plt.bar(xs, explained_var * 100.0)\n",
    "    plt.xticks(xs, [f\"PC{i}\" for i in xs])\n",
    "    plt.ylabel(\"% variance explained\")\n",
    "    plt.xlabel(\"Principal components\")\n",
    "    plt.title(\"PCA variance explained\")\n",
    "    plt.tight_layout()\n",
    "    var_path = os.path.join(out_dir, \"pca_variance_explained.png\")\n",
    "    plt.savefig(var_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # D) t-SNE 2D \n",
    "    tsne_path = None\n",
    "    if HAS_TSNE:\n",
    "        ts = TSNE(n_components=2, random_state=random_state, perplexity=30, init=\"pca\", learning_rate=\"auto\")\n",
    "        Z_tsne = ts.fit_transform(Z_pca)  # use PCA space for stability\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(Z_tsne[:, 0], Z_tsne[:, 1], c=labels, s=18, alpha=0.85)\n",
    "        plt.xlabel(\"t-SNE 1\")\n",
    "        plt.ylabel(\"t-SNE 2\")\n",
    "        plt.title(\"t-SNE (2D) visualization of phenotypes (computed on PCA space) | Cohen's d (PC1): 2.069\")\n",
    "        plt.tight_layout()\n",
    "        tsne_path = os.path.join(out_dir, \"tsne_2d_by_phenotype.png\")\n",
    "        plt.savefig(tsne_path, dpi=250)\n",
    "        plt.close()\n",
    "\n",
    "    # E) UMAP 2D plot \n",
    "    umap_path = None\n",
    "    if HAS_UMAP:\n",
    "        um = umap.UMAP(\n",
    "            n_neighbors=20,\n",
    "            min_dist=0.10,\n",
    "            metric=\"euclidean\",\n",
    "            random_state=random_state\n",
    "        )\n",
    "        Z_umap = um.fit_transform(Zs)\n",
    "\n",
    "        plt.figure(figsize=(9, 7))\n",
    "        plt.scatter(Z_umap[:, 0], Z_umap[:, 1], c=labels, s=18, alpha=0.85)\n",
    "        plt.xlabel(\"UMAP 1\")\n",
    "        plt.ylabel(\"UMAP 2\")\n",
    "        plt.title(\"UMAP phenotypes | Silhouette (PCA space): 0.685\")\n",
    "        plt.tight_layout()\n",
    "        umap_path = os.path.join(out_dir, \"umap_2d_by_phenotype.png\")\n",
    "        plt.savefig(umap_path, dpi=250)\n",
    "        plt.close()\n",
    "\n",
    "    # ----------------------------\n",
    "    # 10) Console output\n",
    "    # ----------------------------\n",
    "    print(\"\\nPhase 3 PCA-then-KMeans phenotyping complete.\")\n",
    "    print(\"Phenotype counts:\", freq.to_dict())\n",
    "    print(\"Saved outputs to:\", out_dir)\n",
    "\n",
    "    return labels_df, summary, pca_scores, freq_df, explained_var, cohens_d, sil\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run (EDIT PATHS)\n",
    "# ----------------------------\n",
    "CLUSTERING_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_clustering_matrix_with_fi.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\"\n",
    "\n",
    "labels_df, summary_df, pca_df, freq_df, explained_var, cohens_d, sil = phase3_pca_then_kmeans_phenotyping_with_plots(\n",
    "    clustering_csv=CLUSTERING_CSV,\n",
    "    out_dir=OUT_DIR,\n",
    "    standardize=True,\n",
    "    k=2,\n",
    "    pca_var=0.90,\n",
    "    n_init=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c62f7dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phenotype labels: (406, 2) | unique: 406\n",
      "phenotype\n",
      "1    326\n",
      "0     80\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Merged -> death_with_phenotype.csv\n",
      "Rows: 406 | unique patients: 406\n",
      "\n",
      "Merged -> severeADR_with_phenotype.csv\n",
      "Rows: 406 | unique patients: 406\n",
      "\n",
      "Merged -> hospitalization_with_phenotype.csv\n",
      "Rows: 406 | unique patients: 406\n",
      "\n",
      "Merged -> master_with_phenotype.csv\n",
      "Rows: 406 | unique patients: 406\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# EDIT THESE PATHS\n",
    "# ----------------------------\n",
    "PHASE3_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\"  # where phenotype_labels_clean.csv was saved\n",
    "PHENO_LABELS = os.path.join(PHASE3_DIR, \"phenotype_labels_clean.csv\")\n",
    "\n",
    "# Main datasets\n",
    "DEATH_DATA = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\death_model_matrix_imputed_v1.csv\"\n",
    "ADR_DATA   = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\severe_adr_model_matrix_imputed_v1.csv\"\n",
    "HOSP_DATA  = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\hospitalization_model_matrix_imputed_v1.csv\"\n",
    "\n",
    "# Optional: your final cleaned master dataset (for clinical characterization later)\n",
    "MASTER_XLSX = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\codige_master_clean__v2.xlsx\"\n",
    "\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\merged_with_phenotypes\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Load phenotype labels\n",
    "# ----------------------------\n",
    "labels = pd.read_csv(PHENO_LABELS)\n",
    "labels.columns = [c.strip() for c in labels.columns]\n",
    "labels[\"patient_id\"] = labels[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "if labels[\"patient_id\"].duplicated().any():\n",
    "    labels = labels.drop_duplicates(\"patient_id\", keep=\"first\").copy()\n",
    "\n",
    "print(\"Phenotype labels:\", labels.shape, \"| unique:\", labels[\"patient_id\"].nunique())\n",
    "print(labels[\"phenotype\"].value_counts(dropna=False))\n",
    "\n",
    "def merge_and_save(path, out_name):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "    merged = df.merge(labels, on=\"patient_id\", how=\"inner\")\n",
    "    merged.to_csv(os.path.join(OUT_DIR, out_name), index=False)\n",
    "\n",
    "    print(f\"\\nMerged -> {out_name}\")\n",
    "    print(\"Rows:\", len(merged), \"| unique patients:\", merged[\"patient_id\"].nunique())\n",
    "    return merged\n",
    "\n",
    "death_merged = merge_and_save(DEATH_DATA, \"death_with_phenotype.csv\")\n",
    "adr_merged   = merge_and_save(ADR_DATA,   \"severeADR_with_phenotype.csv\")\n",
    "hosp_merged  = merge_and_save(HOSP_DATA,  \"hospitalization_with_phenotype.csv\")\n",
    "\n",
    "# Optional: merge phenotype into master dataset for clinical characterization\n",
    "if os.path.exists(MASTER_XLSX):\n",
    "    master = pd.read_excel(MASTER_XLSX)\n",
    "    master.columns = [str(c).strip() for c in master.columns]\n",
    "    master[\"patient_id\"] = master[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "    master_merged = master.merge(labels, on=\"patient_id\", how=\"inner\")\n",
    "    master_merged.to_csv(os.path.join(OUT_DIR, \"master_with_phenotype.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nMerged -> master_with_phenotype.csv\")\n",
    "    print(\"Rows:\", len(master_merged), \"| unique patients:\", master_merged[\"patient_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bcbda600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows: 406\n",
      "\n",
      "Top death_outcome values:\n",
      "death_outcome\n",
      "Absent / No      320\n",
      "Present / Yes     86\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23348\\1620716844.py:53: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  event[s.str.contains(r\"(dead|deceased|death|died|decedut|morto|morta|exitus)\", regex=True, na=False)] = 1\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23348\\1620716844.py:55: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  event[s.str.contains(r\"(alive|living|vivo|viva|censor)\", regex=True, na=False)] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fallback mapping used (frequency rule):\n",
      "  event=0  -> 'Present / Yes' (n=86)\n",
      "  event=1  -> 'Absent / No' (n=320)\n",
      "\n",
      "After time cleaning: 406 dropped: 0\n",
      "After event cleaning: 406 dropped: 0\n",
      "After phenotype cleaning: 406 dropped: 0\n",
      "\n",
      "Final KM dataset:\n",
      "Rows: 406\n",
      "Event counts:\n",
      " event\n",
      "0    320\n",
      "1     86\n",
      "Name: count, dtype: int64\n",
      "Phenotype counts:\n",
      " phenotype\n",
      "1    326\n",
      "0     80\n",
      "Name: count, dtype: int64\n",
      "Age groups: ['<= 65 years', '> 65 years']\n",
      "\n",
      "Saved: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\\km_ready_death_with_phenotype.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "IN_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\merged_with_phenotypes\\death_with_phenotype.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(IN_CSV)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "TIME_COL  = \"survival_days\"\n",
    "EVENT_COL = \"death_outcome\"\n",
    "PHENO_COL = \"phenotype\"\n",
    "AGE_COL   = \"age_group\"\n",
    "\n",
    "print(\"Raw rows:\", len(df))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Clean survival_days\n",
    "# ----------------------------\n",
    "df[TIME_COL] = df[TIME_COL].astype(str).str.strip().str.replace(\",\", \"\", regex=False)\n",
    "df[TIME_COL] = df[TIME_COL].str.extract(r\"(-?\\d+\\.?\\d*)\", expand=False)\n",
    "df[TIME_COL] = pd.to_numeric(df[TIME_COL], errors=\"coerce\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Clean phenotype\n",
    "# ----------------------------\n",
    "df[PHENO_COL] = pd.to_numeric(df[PHENO_COL], errors=\"coerce\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Inspect death_outcome raw values\n",
    "# ----------------------------\n",
    "raw = df[EVENT_COL].astype(str).str.strip()\n",
    "vc = raw.value_counts(dropna=False)\n",
    "\n",
    "print(\"\\nTop death_outcome values:\")\n",
    "print(vc.head(30))\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Robust event mapping\n",
    "# ----------------------------\n",
    "s = raw.str.lower()\n",
    "\n",
    "# keyword-based mapping (English + Italian)\n",
    "death_pat = re.compile(r\"(dead|deceased|death|died|decedut|morto|morta|exitus|si|sì|yes|true|1)$\")\n",
    "alive_pat = re.compile(r\"(alive|living|vivo|viva|no|false|0)$\")\n",
    "\n",
    "event = pd.Series(np.nan, index=df.index, dtype=\"float\")\n",
    "\n",
    "# mark deaths\n",
    "event[s.str.contains(r\"(dead|deceased|death|died|decedut|morto|morta|exitus)\", regex=True, na=False)] = 1\n",
    "# mark alive/censored\n",
    "event[s.str.contains(r\"(alive|living|vivo|viva|censor)\", regex=True, na=False)] = 0\n",
    "\n",
    "# numeric-like direct mapping (handles \"1\", \"0\", \"1.0\", \"0.0\", \"2\", etc.)\n",
    "num_try = pd.to_numeric(s.str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "# if numeric and not yet assigned\n",
    "event[event.isna() & num_try.notna()] = num_try[event.isna() & num_try.notna()]\n",
    "\n",
    "# If still not 0/1, but exactly 2 categories exist -> use frequency rule\n",
    "tmp = pd.to_numeric(event, errors=\"coerce\")\n",
    "ok = tmp.dropna()\n",
    "\n",
    "if not set(ok.unique()).issubset({0, 1}):\n",
    "    # try min/max mapping if there are exactly two unique numbers (e.g., 1/2)\n",
    "    uniq = sorted(ok.unique().tolist())\n",
    "    if len(uniq) == 2:\n",
    "        mn, mx = uniq[0], uniq[1]\n",
    "        tmp = (tmp == mx).astype(float)  # max becomes 1\n",
    "        event = tmp\n",
    "\n",
    "# If STILL NaN-heavy, use category frequency assumption (smallest group = event)\n",
    "tmp2 = pd.to_numeric(event, errors=\"coerce\")\n",
    "if tmp2.isna().mean() > 0.20:\n",
    "    # fallback: treat the rarer of the two most common raw categories as event=1\n",
    "    top_vals = vc.index.astype(str).tolist()\n",
    "    # keep only real values (not 'nan')\n",
    "    top_vals = [v for v in top_vals if v.lower() not in [\"nan\", \"none\", \"\"]]\n",
    "    if len(top_vals) >= 2:\n",
    "        v1, v2 = top_vals[0], top_vals[1]\n",
    "        c1, c2 = vc.loc[v1], vc.loc[v2]\n",
    "        event_val = v1 if c1 < c2 else v2\n",
    "        censor_val = v2 if event_val == v1 else v1\n",
    "\n",
    "        event = np.where(raw == event_val, 1,\n",
    "                 np.where(raw == censor_val, 0, np.nan)).astype(float)\n",
    "\n",
    "        print(f\"\\nFallback mapping used (frequency rule):\")\n",
    "        print(f\"  event=0  -> '{event_val}' (n={min(c1,c2)})\")\n",
    "        print(f\"  event=1  -> '{censor_val}' (n={max(c1,c2)})\")\n",
    "\n",
    "df[\"event\"] = pd.to_numeric(event, errors=\"coerce\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Final KM-ready filter\n",
    "# ----------------------------\n",
    "before = len(df)\n",
    "df = df[df[TIME_COL].notna() & (df[TIME_COL] >= 0)].copy()\n",
    "print(\"\\nAfter time cleaning:\", len(df), \"dropped:\", before - len(df))\n",
    "\n",
    "before = len(df)\n",
    "df = df[df[\"event\"].isin([0, 1])].copy()\n",
    "print(\"After event cleaning:\", len(df), \"dropped:\", before - len(df))\n",
    "\n",
    "before = len(df)\n",
    "df = df[df[PHENO_COL].isin([0, 1])].copy()\n",
    "print(\"After phenotype cleaning:\", len(df), \"dropped:\", before - len(df))\n",
    "\n",
    "df[\"time\"] = df[TIME_COL].astype(float)\n",
    "df[\"event\"] = df[\"event\"].astype(int)\n",
    "df[\"phenotype\"] = df[PHENO_COL].astype(int)\n",
    "df[\"age_group_clean\"] = df[AGE_COL].astype(str).str.strip()\n",
    "\n",
    "print(\"\\nFinal KM dataset:\")\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Event counts:\\n\", df[\"event\"].value_counts(dropna=False))\n",
    "print(\"Phenotype counts:\\n\", df[\"phenotype\"].value_counts(dropna=False))\n",
    "print(\"Age groups:\", sorted(df[\"age_group_clean\"].dropna().unique().tolist()))\n",
    "\n",
    "km_ready_path = os.path.join(OUT_DIR, \"km_ready_death_with_phenotype.csv\")\n",
    "df.to_csv(km_ready_path, index=False)\n",
    "print(\"\\nSaved:\", km_ready_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5caf0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KM plots to: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "IN_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\\km_ready_death_with_phenotype.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(IN_CSV)\n",
    "\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# 1) Overall by phenotype\n",
    "plt.figure(figsize=(8, 6))\n",
    "for ph in [0, 1]:\n",
    "    sub = df[df[\"phenotype\"] == ph]\n",
    "    label = \"Decelerated aging\" if ph == 1 else \"Accelerated aging\"\n",
    "    kmf.fit(sub[\"time\"], event_observed=sub[\"event\"], label=label)\n",
    "    kmf.plot_survival_function(ci_show=True)\n",
    "\n",
    "plt.title(\"Kaplan–Meier Survival by Aging Phenotype\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"km_overall_by_phenotype.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 2) Panels by age group\n",
    "age_groups = sorted(df[\"age_group_clean\"].dropna().unique().tolist())\n",
    "n = len(age_groups)\n",
    "\n",
    "fig, axes = plt.subplots(1, n, figsize=(6*n, 5), sharey=True)\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, ag in zip(axes, age_groups):\n",
    "    sub_ag = df[df[\"age_group_clean\"] == ag]\n",
    "    for ph in [0, 1]:\n",
    "        sub = sub_ag[sub_ag[\"phenotype\"] == ph]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        label = \"Decelerated\" if ph == 1 else \"Accelerated\"\n",
    "        kmf.fit(sub[\"time\"], event_observed=sub[\"event\"], label=label)\n",
    "        kmf.plot_survival_function(ax=ax, ci_show=True)\n",
    "    ax.set_title(f\"Age group: {ag}\")\n",
    "    ax.set_xlabel(\"Time (days)\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "axes[0].set_ylabel(\"Survival probability\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"km_by_agegroup_and_phenotype.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 3) 4 groups single panel\n",
    "plt.figure(figsize=(9, 6))\n",
    "for ag in age_groups:\n",
    "    for ph in [0, 1]:\n",
    "        sub = df[(df[\"age_group_clean\"] == ag) & (df[\"phenotype\"] == ph)]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        label = f\"{ag} | {'Decelerated' if ph == 0 else 'Accelerated'}\"\n",
    "        kmf.fit(sub[\"time\"], event_observed=sub[\"event\"], label=label)\n",
    "        kmf.plot_survival_function(ci_show=False)\n",
    "\n",
    "plt.title(\"Kaplan–Meier Survival by Phenotype and Age Group\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"km_4groups_singlepanel.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved KM plots to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e6523f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\\clinical_characterization_continuous.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "df = pd.read_csv(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\\km_ready_death_with_phenotype.csv\")\n",
    "\n",
    "\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Map phenotype labels (adjust if your mapping is reversed)\n",
    "df[\"phenotype\"] = df[\"phenotype\"].map({0: \"Decelerated aging\", 1: \"Accelerated aging\"})\n",
    "\n",
    "continuous_vars = [\n",
    "    \"age\",\n",
    "    \"cci_score\",\n",
    "    \"adr_n_tot\",\n",
    "    \"total_chemo_cycles\",\n",
    "    \"treatment_duration_days\",\n",
    "    \"time_from_diagnosis_to_first_treatment_days\"\n",
    "]\n",
    "continuous_vars = [v for v in continuous_vars if v in df.columns]\n",
    "\n",
    "# force numeric for continuous variables\n",
    "for c in continuous_vars:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "def median_iqr(x):\n",
    "    x = x.dropna()\n",
    "    if len(x) == 0:\n",
    "        return pd.Series({\"median\": np.nan, \"q1\": np.nan, \"q3\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"median\": x.median(),\n",
    "        \"q1\": x.quantile(0.25),\n",
    "        \"q3\": x.quantile(0.75)\n",
    "    })\n",
    "\n",
    "cont_table = df.groupby(\"phenotype\")[continuous_vars].apply(lambda g: g.apply(median_iqr)).reset_index()\n",
    "\n",
    "# Make the table wide: one row per variable\n",
    "cont_table = cont_table.melt(id_vars=[\"phenotype\", \"level_1\"], var_name=\"stat\", value_name=\"value\")\n",
    "cont_table = cont_table.pivot_table(index=\"level_1\", columns=[\"phenotype\", \"stat\"], values=\"value\")\n",
    "\n",
    "# flatten columns\n",
    "cont_table.columns = [f\"{pheno}_{stat}\" for pheno, stat in cont_table.columns]\n",
    "cont_table = cont_table.reset_index().rename(columns={\"level_1\": \"variable\"})\n",
    "\n",
    "cont_table.to_csv(\"clinical_characterization_continuous.csv\", index=False)\n",
    "print(\"Saved: clinical_characterization_continuous.csv\")\n",
    "print(cont_table.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "459b61e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\\clinical_characterization_binary.csv\n"
     ]
    }
   ],
   "source": [
    "binary_vars = [\n",
    "    \"death_outcome\",\n",
    "    \"received_chemo\",\n",
    "    \"any_dose_reduction\",\n",
    "    \"any_toxicity\",\n",
    "    \"end_due_to_progression\",\n",
    "    \"hypertension\",\n",
    "    \"renal_insufficiency\",\n",
    "    \"atrial_fibrillation\",\n",
    "    \"diabete_tipo_II\",\n",
    "    \"anemia_comorbidity\",\n",
    "    \"polypharmacy_flag\"\n",
    "]\n",
    "binary_vars = [v for v in binary_vars if v in df.columns]\n",
    "\n",
    "# Make sure binaries are numeric 0/1 (handles True/False, \"Yes\"/\"No\", etc.)\n",
    "def to_binary(series):\n",
    "    s = series.copy()\n",
    "    if s.dtype == bool:\n",
    "        return s.astype(int)\n",
    "    s = s.astype(str).str.strip().str.lower()\n",
    "    s = s.replace({\n",
    "        \"1\": 1, \"0\": 0,\n",
    "        \"true\": 1, \"false\": 0,\n",
    "        \"yes\": 1, \"no\": 0,\n",
    "        \"y\": 1, \"n\": 0\n",
    "    })\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "for c in binary_vars:\n",
    "    df[c] = to_binary(df[c])\n",
    "\n",
    "rows = []\n",
    "for var in binary_vars:\n",
    "    tmp = df.groupby(\"phenotype\")[var].agg([\"sum\", \"count\"]).reset_index()\n",
    "    tmp[\"percent\"] = 100 * tmp[\"sum\"] / tmp[\"count\"]\n",
    "    tmp[\"variable\"] = var\n",
    "    rows.append(tmp)\n",
    "\n",
    "bin_table = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "bin_wide = bin_table.pivot(index=\"variable\", columns=\"phenotype\", values=[\"sum\", \"percent\"])\n",
    "bin_wide.columns = [f\"{stat}_{pheno}\" for stat, pheno in bin_wide.columns]\n",
    "bin_wide = bin_wide.reset_index()\n",
    "\n",
    "bin_path = os.path.join(OUT_DIR, \"clinical_characterization_binary.csv\")\n",
    "bin_wide.to_csv(bin_path, index=False)\n",
    "print(\"Saved:\", bin_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318ae38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

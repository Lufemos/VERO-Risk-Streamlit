{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686c21b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated columns: []\n",
      "(406, 77)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load all three model matrices\n",
    "death_df = pd.read_csv(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\death_model_matrix_imputed_v1.csv\")\n",
    "hosp_df = pd.read_csv(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\hospitalization_model_matrix_imputed_v1.csv\")\n",
    "adr_df = pd.read_csv(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\severe_adr_model_matrix_imputed_v1.csv\")\n",
    "\n",
    "# 1) Start from death_df as the canonical analytic dataset\n",
    "integrated = death_df.copy()\n",
    "\n",
    "# 2) Define ONLY the extra columns you want from each additional matrix\n",
    "#    (everything else will come from death_df to avoid triplication)\n",
    "\n",
    "hosp_outcome_cols = [\n",
    "    \"hospitalization_flag\",\n",
    "]\n",
    "\n",
    "adr_outcome_cols = [\n",
    "    \"severe_adr_flag\",\n",
    "]\n",
    "\n",
    "# 3) Filter these lists to:\n",
    "#    - keep only columns that actually exist in the respective df\n",
    "#    - avoid any column names that are already present in `integrated`\n",
    "\n",
    "hosp_outcome_cols = [\n",
    "    c for c in hosp_outcome_cols\n",
    "    if c in hosp_df.columns and c not in integrated.columns\n",
    "]\n",
    "\n",
    "adr_outcome_cols = [\n",
    "    c for c in adr_outcome_cols\n",
    "    if c in adr_df.columns and c not in integrated.columns\n",
    "]\n",
    "\n",
    "# 4) Merge ONLY patient_id + these new outcome columns\n",
    "\n",
    "if hosp_outcome_cols:\n",
    "    integrated = integrated.merge(\n",
    "        hosp_df[[\"patient_id\"] + hosp_outcome_cols],\n",
    "        on=\"patient_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "if adr_outcome_cols:\n",
    "    integrated = integrated.merge(\n",
    "        adr_df[[\"patient_id\"] + adr_outcome_cols],\n",
    "        on=\"patient_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "# 5) Final sanity check for any duplicated columns\n",
    "dup_cols = integrated.columns[integrated.columns.duplicated()].tolist()\n",
    "print(\"Duplicated columns:\", dup_cols)\n",
    "\n",
    "# If this prints an empty list, you are safe.\n",
    "# If anything shows up, you can decide which version to keep.\n",
    "# But with the filters above it should normally be [].\n",
    "\n",
    "print(integrated.shape)\n",
    "integrated.to_csv(\"phase3_integrated_base.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299986d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example if you ever see suffixed columns\n",
    "cols_to_drop = [c for c in integrated.columns if c.endswith(\"_y\")]\n",
    "integrated = integrated.drop(columns=cols_to_drop)\n",
    "\n",
    "# And optionally rename *_x columns back to base names:\n",
    "integrated.columns = [\n",
    "    c[:-2] if c.endswith(\"_x\") else c\n",
    "    for c in integrated.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b871038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Death univariate (with effect size)\n",
    "uni_death = pd.read_excel(r\"C:/Users\\HP/OneDrive/Desktop/Results/binary_models/univariate_screening_death_death_outcome_with_effect_size.xlsx\")\n",
    "\n",
    "# Normalise column names\n",
    "uni_death_norm = uni_death.rename(columns={\n",
    "    \"Test_Used\": \"Test Used\",\n",
    "    \"p_value\": \"P-Value\",\n",
    "    \"Effect_Size\": \"Effect Size\",\n",
    "    \"p_value_fdr_bh\": \"Corrected P-Value\"\n",
    "})\n",
    "# Make sure Outcome is consistently labelled\n",
    "uni_death_norm[\"Outcome\"] = \"Death\"\n",
    "\n",
    "# Hospitalization\n",
    "uni_hosp = pd.read_excel(\"C:/Users/HP/OneDrive/Desktop/Results/binary_models/univariate_screening_hospitalization_with_effects.xlsx\")\n",
    "uni_hosp_norm = uni_hosp.copy()  # already has Outcome / Feature / etc.\n",
    "uni_hosp_norm[\"Outcome\"] = \"Hospitalization\"\n",
    "\n",
    "# Severe ADR\n",
    "uni_sev = pd.read_excel(\"C:/Users/HP/OneDrive/Desktop/Results/binary_models/univariate_screening_severeADR_with_effects.xlsx\")\n",
    "uni_sev_norm = uni_sev.copy()\n",
    "uni_sev_norm[\"Outcome\"] = \"Severe_ADR\"\n",
    "\n",
    "# Stack them into one long table\n",
    "uni_all = pd.concat(\n",
    "    [uni_death_norm, uni_hosp_norm, uni_sev_norm],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Keep only the columns we need\n",
    "uni_all = uni_all[[\"Outcome\", \"Feature\", \"Test Used\", \"P-Value\", \"Effect Size\", \"Corrected P-Value\"]]\n",
    "\n",
    "uni_all.to_csv(\"phase3_univariate_all_outcomes.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4bb1557",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/HP/OneDrive/Desktop/Results/binary_models/logreg_multivariable_Hospitalization_coefficients.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m coef_death_norm[\u001b[33m\"\u001b[39m\u001b[33mOutcome\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mDeath\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Hospitalization logistic model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m coef_hosp = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:/Users/HP/OneDrive/Desktop/Results/binary_models/logreg_multivariable_Hospitalization_coefficients.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m coef_hosp_norm = coef_hosp.copy()\n\u001b[32m     14\u001b[39m coef_hosp_norm[\u001b[33m\"\u001b[39m\u001b[33mOutcome\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mHospitalization\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Users/HP/OneDrive/Desktop/Results/binary_models/logreg_multivariable_Hospitalization_coefficients.xlsx'"
     ]
    }
   ],
   "source": [
    "# Death: use lambda_ (time scaling) coefficients as the main effect\n",
    "aft_coef = pd.read_excel(\"C:/Users/HP/OneDrive/Desktop/Results/Death Outcome Models/weibull_aft_death_model_summary_covset.xlsx\")\n",
    "coef_death = aft_coef[aft_coef[\"param\"] == \"lambda_\"].copy()\n",
    "\n",
    "coef_death_norm = coef_death.rename(columns={\n",
    "    \"covariate\": \"feature\",\n",
    "    \"Time_Ratio\": \"OR\"   \n",
    "})\n",
    "coef_death_norm[\"Outcome\"] = \"Death\"\n",
    "\n",
    "# Hospitalization logistic model\n",
    "coef_hosp = pd.read_excel(\"C:/Users/HP/OneDrive/Desktop/Results/binary_models/logreg_multivariable_Hospitalization_coefficients.xlsx\")\n",
    "coef_hosp_norm = coef_hosp.copy()\n",
    "coef_hosp_norm[\"Outcome\"] = \"Hospitalization\"\n",
    "\n",
    "# Severe ADR logistic model\n",
    "coef_sev = pd.read_excel(\"C:/Users/HP/OneDrive/Desktop/Results/binary_models/logreg_multivariable_Severe_ADR_coefficients.xlsx\")\n",
    "coef_sev_norm = coef_sev.copy()\n",
    "coef_sev_norm[\"Outcome\"] = \"Severe_ADR\"\n",
    "\n",
    "# Align column names and stack\n",
    "coef_all = pd.concat([\n",
    "    coef_death_norm[[\"feature\", \"coef\", \"OR\", \"Outcome\"]],\n",
    "    coef_hosp_norm[[\"feature\", \"coef\", \"OR\", \"Outcome\"]],\n",
    "    coef_sev_norm[[\"feature\", \"coef\", \"OR\", \"Outcome\"]],\n",
    "], ignore_index=True)\n",
    "\n",
    "coef_all.to_csv(\"phase3_coefficients_all_outcomes.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_death = pd.read_excel(\"C:/Users/HP/OneDrive/Desktop/Results/survival_models/RI_Significance_Table_robust_death.xlsx\")\n",
    "# We will mostly need: variable, Combined_RI, Mean_log_effect\n",
    "ri_death_small = ri_death[[\"variable\", \"Combined_RI\", \"Mean_log_effect\"]].copy()\n",
    "ri_death_small.to_csv(\"phase3_ri_death_simple.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be98bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load integrated base dataset\n",
    "integrated = pd.read_csv(\"C:/Users/HP/OneDrive/Desktop/phase3_phenotypes/phase3_integrated_base.csv\")\n",
    "\n",
    "# 2. Define which columns are *not* candidate features\n",
    "non_features = {\n",
    "    \"patient_id\",\n",
    "    \"death_outcome\",\n",
    "    \"survival_days\",\n",
    "    \"hospitalization_flag\",\n",
    "    \"severe_adr_flag\",\n",
    "}\n",
    "\n",
    "candidate_features = [c for c in integrated.columns if c not in non_features]\n",
    "\n",
    "# 3. Load normalised univariate and coefficient tables, and RI\n",
    "uni_all = pd.read_csv(\"C:/Users/HP/OneDrive/Desktop/phase3_phenotypes/phase3_univariate_all_outcomes.csv\")\n",
    "coef_all = pd.read_csv(\"C:/Users/HP/OneDrive/Desktop/phase3_phenotypes/phase3_coefficients_all_outcomes.csv\")\n",
    "ri_death_small = pd.read_csv(\"C:/Users/HP/OneDrive/Desktop/phase3_phenotypes/phase3_ri_death_simple.csv\")\n",
    "\n",
    "# 4. Build an empty meta table over candidate features\n",
    "meta = pd.DataFrame({\"feature\": candidate_features})\n",
    "\n",
    "# Example: link RI by exact match between feature name and 'variable'\n",
    "meta = meta.merge(\n",
    "    ri_death_small.rename(columns={\"variable\": \"feature\"}),\n",
    "    on=\"feature\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5. Aggregate univariate stats per feature across outcomes\n",
    "uni_agg = (\n",
    "    uni_all.groupby(\"Feature\")\n",
    "           .agg(\n",
    "               min_p=(\"Corrected P-Value\", \"min\"),\n",
    "               max_abs_effect=(\"Effect Size\", lambda x: np.nanmax(np.abs(x)))\n",
    "           )\n",
    "           .reset_index()\n",
    "           .rename(columns={\"Feature\": \"feature\"})\n",
    ")\n",
    "\n",
    "meta = meta.merge(uni_agg, on=\"feature\", how=\"left\")\n",
    "\n",
    "# 6. Aggregate coefficients per feature across outcomes\n",
    "coef_agg = (\n",
    "    coef_all.groupby(\"feature\")\n",
    "            .agg(\n",
    "                max_abs_coef=(\"coef\", lambda x: np.nanmax(np.abs(x))),\n",
    "                max_or=(\"OR\", \"max\")\n",
    "            )\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "meta = meta.merge(coef_agg, on=\"feature\", how=\"left\")\n",
    "\n",
    "# 7. Normalise each metric to [0, 1] for scoring\n",
    "def to_0_1(series):\n",
    "    s = series.copy()\n",
    "    s = (s - s.min()) / (s.max() - s.min()) if s.notna().any() and s.min() != s.max() else s * 0.0\n",
    "    return s\n",
    "\n",
    "meta[\"score_RI\"] = to_0_1(meta[\"Combined_RI\"])\n",
    "meta[\"score_effect\"] = to_0_1(meta[\"max_abs_effect\"])\n",
    "meta[\"score_coef\"] = to_0_1(meta[\"max_abs_coef\"])\n",
    "\n",
    "# For p-values, lower is better, so invert\n",
    "meta[\"score_p\"] = to_0_1(-np.log10(meta[\"min_p\"]))\n",
    "\n",
    "# 8. Combine scores (you can adjust weights to match your old notebook)\n",
    "meta[\"feature_score\"] = (\n",
    "    0.35 * meta[\"score_RI\"] +\n",
    "    0.25 * meta[\"score_effect\"] +\n",
    "    0.25 * meta[\"score_coef\"] +\n",
    "    0.15 * meta[\"score_p\"]\n",
    ")\n",
    "\n",
    "# 9. Sort and inspect top features\n",
    "meta_sorted = meta.sort_values(\"feature_score\", ascending=False)\n",
    "meta_sorted.to_csv(\"phase3_feature_meta_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b9e721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45,\n",
       " ['surgical_intervention',\n",
       "  'end_reason_progression_any_line',\n",
       "  'genotipo_DPYD_type',\n",
       "  'end_due_to_progression',\n",
       "  'adr_description',\n",
       "  'received_targeted_therapy',\n",
       "  'dyslipidemia',\n",
       "  'cardiovascular_disorders',\n",
       "  'cci_score',\n",
       "  'total_unique_active_drugs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_N = 45\n",
    "always_include = [\"age_group\", \"FI_LAB\"]  # expand this with your must-have variables\n",
    "\n",
    "top_features = meta_sorted[\"feature\"].head(TOP_N).tolist()\n",
    "for col in always_include:\n",
    "    if col in candidate_features and col not in top_features:\n",
    "        top_features.append(col)\n",
    "\n",
    "len(top_features), top_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8a4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_cols = [\"patient_id\", \"death_outcome\", \"survival_days\",\n",
    "                  \"hospitalization_flag\", \"severe_adr_flag\"] + top_features\n",
    "\n",
    "phenotype_df = integrated[phenotype_cols].copy()\n",
    "phenotype_df.to_csv(\"phase3_phenotyping_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db6a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paths (edit as needed)\n",
    "BASE_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "death_df = pd.read_csv(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\death_model_matrix_imputed_v1.csv\")\n",
    "hosp_df  = pd.read_csv(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\hospitalization_model_matrix_imputed_v1.csv\")\n",
    "adr_df   = pd.read_csv(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\severe_adr_model_matrix_imputed_v1.csv\")\n",
    "\n",
    "cox_coef = pd.read_excel(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 2\\Death Outcome Model\\survival_models\\coxph_death_model_summary_penalized.xlsx\")\n",
    "hosp_coef = pd.read_excel(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 2\\Binary Models\\logreg_multivariable_Hospitalization_coefficients.xlsx\")\n",
    "adr_coef  = pd.read_excel(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 2\\Binary Models\\logreg_multivariable_Severe_ADR_coefficients.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca5532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_to_series(df: pd.DataFrame, feat: str):\n",
    "    feat = str(feat)\n",
    "\n",
    "    # direct numeric/binary column\n",
    "    if feat in df.columns:\n",
    "        return pd.to_numeric(df[feat], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # Cox-style dummy: var=value\n",
    "    if \"=\" in feat:\n",
    "        base, level = feat.split(\"=\", 1)\n",
    "        base = base.strip()\n",
    "        level = level.strip()\n",
    "        if base in df.columns:\n",
    "            return (df[base].astype(str).str.strip() == level).astype(int)\n",
    "\n",
    "    # Logistic-style dummy: var_level (but var itself may contain underscores)\n",
    "    candidates = [c for c in df.columns if isinstance(c, str) and feat.startswith(c + \"_\")]\n",
    "    if candidates:\n",
    "        base = max(candidates, key=len)  # longest matching prefix\n",
    "        level = feat[len(base) + 1:].strip()\n",
    "        return (df[base].astype(str).str.strip() == level).astype(int)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf7b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing death features: 0\n",
      "Missing hosp features: 0\n",
      "Missing adr features: 0\n"
     ]
    }
   ],
   "source": [
    "def compute_score(df, feat_col, coef_col, id_col=\"patient_id\"):\n",
    "    score = np.zeros(len(df), dtype=float)\n",
    "    missing = []\n",
    "\n",
    "    for feat, coefv in zip(feat_col.astype(str), coef_col.astype(float)):\n",
    "        s = feature_to_series(df, feat)\n",
    "        if s is None:\n",
    "            missing.append(feat)\n",
    "            continue\n",
    "        score += coefv * s.values\n",
    "\n",
    "    out = pd.DataFrame({id_col: df[id_col].values, \"score\": score})\n",
    "    return out, missing\n",
    "\n",
    "death_score, miss_death = compute_score(death_df, cox_coef[\"covariate\"], cox_coef[\"coef\"])\n",
    "hosp_score,  miss_hosp  = compute_score(hosp_df, hosp_coef[\"feature\"], hosp_coef[\"coef\"])\n",
    "adr_score,   miss_adr   = compute_score(adr_df,  adr_coef[\"feature\"],  adr_coef[\"coef\"])\n",
    "\n",
    "print(\"Missing death features:\", len(miss_death))\n",
    "print(\"Missing hosp features:\", len(miss_hosp))\n",
    "print(\"Missing adr features:\", len(miss_adr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c41b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick baseline columns that help interpretation later (safe baseline anchors)\n",
    "base_cols = [\"patient_id\", \"age\", \"age_group\", \"gender\", \"tumor_type\", \"cci_score\"]\n",
    "\n",
    "phase3_base = death_df[base_cols].copy()\n",
    "\n",
    "phase3_base = phase3_base.merge(death_score, on=\"patient_id\", how=\"left\").rename(columns={\"score\":\"risk_death\"})\n",
    "phase3_base = phase3_base.merge(hosp_score,  on=\"patient_id\", how=\"left\").rename(columns={\"score\":\"risk_hosp\"})\n",
    "phase3_base = phase3_base.merge(adr_score,   on=\"patient_id\", how=\"left\").rename(columns={\"score\":\"risk_adr\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c778a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_integrated_base_clean.csv\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "phase3_base[[\"z_risk_death\",\"z_risk_hosp\",\"z_risk_adr\"]] = scaler.fit_transform(\n",
    "    phase3_base[[\"risk_death\",\"risk_hosp\",\"risk_adr\"]]\n",
    ")\n",
    "\n",
    "phase3_base.to_csv(os.path.join(BASE_DIR, \"phase3_integrated_base_clean.csv\"), index=False)\n",
    "print(\"Saved:\", os.path.join(BASE_DIR, \"phase3_integrated_base_clean.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958cb7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_clustering_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "cluster_features = [\"z_risk_death\",\"z_risk_hosp\",\"z_risk_adr\"]\n",
    "X_cluster = phase3_base[[\"patient_id\"] + cluster_features].copy()\n",
    "X_cluster.to_csv(os.path.join(BASE_DIR, \"phase3_clustering_matrix.csv\"), index=False)\n",
    "\n",
    "print(\"Saved:\", os.path.join(BASE_DIR, \"phase3_clustering_matrix.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab301542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential leakage columns in phase3_base: []\n"
     ]
    }
   ],
   "source": [
    "LEAK_PAT = re.compile(r\"(outcome|survival|death_outcome|hospitalization_flag|severe_adr_flag|adr_n_|grado|count|event|time_)\", re.I)\n",
    "\n",
    "bad_cols = [c for c in phase3_base.columns if LEAK_PAT.search(c)]\n",
    "print(\"Potential leakage columns in phase3_base:\", bad_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a44b311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_outcomes_for_validation_only.csv\n"
     ]
    }
   ],
   "source": [
    "eval_df = death_df[[\"patient_id\",\"death_outcome\",\"survival_days\"]].copy() \\\n",
    "    .merge(hosp_df[[\"patient_id\",\"hospitalization_flag\"]], on=\"patient_id\", how=\"left\") \\\n",
    "    .merge(adr_df[[\"patient_id\",\"severe_adr_flag\"]], on=\"patient_id\", how=\"left\")\n",
    "\n",
    "eval_df.to_csv(os.path.join(BASE_DIR, \"phase3_outcomes_for_validation_only.csv\"), index=False)\n",
    "print(\"Saved:\", os.path.join(BASE_DIR, \"phase3_outcomes_for_validation_only.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9921cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
